# 2019-09-18-케라스 창시자에게 배우는 딥러닝 -1부-

2019.09.18 시작

## &#128214; 1부

### &#128218; 1장. 딥러닝이란 무엇일까?

#### 1-1. 용어 정리

1. 인공 지능 : 보통의 사람이 수행하는 지능적인 작업을 자동화하기 위한 연구활동
   충분히 많은 규칙 정의 -> <b>심볼릭 AI</b>
   복잡한 문제의 규칙은 찾기 어렵다.
2. 머신러닝 : 코드로 정의하지 않은 동작을 실행하는 능력에 댛나 연구활동.
   심볼릭 AI와 달리 '규칙'을 찾아내는데 집중.
   필요한 것 : 입력 데이터, 기대 출력, 성능 측정 방법.
   즉, 입력 데이터를 <b>의미 있는 데이터(=표현, representation)</b>로의 변환 작업 수행
   학습은 정의된 변환에 최적화시키는 것.
   다만 머신러닝이 이런 변환을 찾는 능력은 없다.
   가설 공간(hypothesis space)라 부르는 미리 정의된 연산의 집합을 조사할 뿐
3. 딥러닝 : 층 기반 표현학습(layered representations learning) 또는 계층적 표현학습(hierarchical representations learning)
   층(layer)을 이용해 표현을 학습. 이 층이 연속적으로 나열되어 정보가 어떤 작업에 대해 유용하게 정제되는 작업.
   즉, <b>머신러닝 : 입력 데이터 ->(mapping) -> 타겟 데이터

#### 1-2. 딥러닝 이전

1. 확률적 모델링 : 통계학 이론을 데이터 분석에 응용
   - Naive Bayes, Logistic regression.
2. 초창기 신경망 : LeNet. 초창기 CNN
3. 커널 방법 : 분류 알고리즘의 종류. SVM이 대표적.
   좋은 결정 경계(decision boundary)를 찾는게 목표.
   데이터를 고차원으로 매핑하는 기법이 어려워서 '커널 기법'이용. 커널 함수를 통해 원본 공간 상의 두 데이터를 타깃 공간에서의 거리를 매핑함.
4. 결정 트리, 랜덤 포레스트, 그레디언트 부스팅 머신 : 플로 차트 같은 구조. 이 구조를 여러개 만들고 출력을 앙상블하면 랜덤 포레스트. 그레디언트 부스팅 머신은 비슷하게 약한 예측 모델인 결정 트리를 앙상블해서 사용.
   이전 모델에서 놓친 데이터를 보완하는 새로운 모델을 반복적으로 훈련해서 모델 성능을 올리는 그레디언트 부스팅을 사용함.

=> 딥러닝은 특성 공학(유용한 표현 추출)을 완전 자동화.
<b>End-to-end</b>. 모든 층이 순차적이지 않고 동시에 학습된다. 그래서 얕은 모델을 탐욕적(순차적)으로 쌓은 것 보다 좋다.

&#128204;<b>특징</b> 

1. 층을 거치면서 점진적으로 더 복잡한 표현 생성
2. 점진적인 중간 표현이 공동으로 학습.

#### 1-3. 최근 동향

캐글의 머신러닝 경연을 보면 어떤 것이 좋은지 알 수 있음. 대체로 구조적인 데이터는 그레디언트 부스팅을, 이미지 분류 같은 지각에 관한 문제는 딥러닝으로 해결. (XGBoost 라이브러리, Keras)

### &#128218; 2장. 시작하기 전에: 신경망의 수학적 구성 요소.

#### 2-1. 신경망과의 첫 만남.

대표적인 연습 데이터셋인 MNIST 사용.
train_data 6만, test_data 1만, Class 10개

```python
from keras import models
from keras import layers

network = models.Sequential()
network.add(layers.Dense(512, activation='relu', input_shape=(28*28,)))
network.add(layers.Dense(10, activation='softmax'))
```

이렇게 모델 구성 후, 학습에 필요한 손실함수, 옵티마이저, 모니터링 지표를 정의.

```python
network.compile(optimizer='rmsprop',
                loss='categorical_crossentropy',
                metrics=['accuracy'])
```

또한 입력데이터 전처리 진행. 신경망은 입력 데이터의 스케일에 민감해서 일정한 스케일을 가지도록 cross entropy를 사용하기 위해 one_hot_encoding을 keras.util.to_categorical(labels)로 사용해서 만든다.

#### 2-2. 신경망을 위한 데이터 표현

1. 텐서 : 데이터를 위한 컨테이너. 임의의 차원 개수를 가지는 행렬(차원을 텐서에서는 종종 축(axis)로 부름)
   - 스칼라(0D 텐서) : 텐서 축 개수 0. 하나의 숫자.
   - 벡터 (1D 텐서) : 숫자 배열.
   - 행렬 (2D 텐서) : 행렬
   - ...
2. 텐서의 속성
   - 축의 개수(랭크)
   - 크기(shape) : 튜플로 표현
   - 데이터 타입
3. 배치 데이터
   일반적으로 딥러닝에서 사용하는 데이터 텐서의 첫번째 축은 샘플 축. 데이터 양이 너무 많아서 한 번에 처리핮 ㅣ않고 배치(batch)단위로 사용. 이런 배치 데이터를 다룰 때 첫 번째 축을 '배치 축' 또는 '배치 차원(batch dimension)'이라 부름.
   ex) 이미지 : (samples, height, width, channels)
         동영상 : (samples, frames, height, width, channels)

> <b>매니폴드(manifold)</b> : 국부적으로는 저차원 유클리디안 거리로 볼 수 있는 고차원 공간.

#### 2-3. 신경망의 엔진 : 그레디언트 기반 최적화.

1. 훈련 반복 루프

   - 배치 추출
   - 모델 실행, 예측값 구하기
   - 정답과의 차이를 측정해서 손실 계산
   - 손실이 줄어들도록 가중치 학습

2. 확률적 경사 하강법(SGD, Stochastic Gradient Descent)

   $$W_{i+1} = W_i - learningRate * gradient(cost function)(W_i)$$

   
   이 과정을 배치에 대해 학습할 때마다 적용.
   이를 '미니 배치 확률적 경사 하강법'이라고 함.

   > '확률적'이란 것은 '무작위'의 과학적 표현

   여기서 더 나아가 이전 업데이트된 가중치도 고려하는 등 발전도니 방식 존재. 이런 방법들을 '최적화 방법(optimization method)'또는 '옵티마이저'라고 함.

   > <b>모멘텀(momentum)</b>
   >
   > SGD의 2가지 문제점인 수렴 속도와 지역 최솟값(local minima)를 어느정도 해결
   >
   > ```python
   > past_celocity = 0.
   > momentum = 0.1     # 모멘텀 상수
   > while loss > 0.01: # 최적화 반복 루프
   >     w, loss, gradient = get_current_parameters()
   >     velocity = momentum * past_velocity - learning_rate * gradient
   >     w = w + momentum * velocity - learning_rate * gradient
   >     past_velocity = velocity
   >     update_parameter(w)
   > ```

   마지막으로 훈련 반복

   ```python
   network.fit(train_images, 
               train_labels, 
               epochs=5,        # 에폭 : 전체 데이터를 한 번 학습. 단위
               batch_size=128)
   ```

### &#128218; 3장. 신경망 시작하기

#### 3-1. 신경망의 구조

- 모델을 구성하는 층(layer)
- 입력 데이터와 타겟
- 손실 함수
- 옵티마이저

1. layer.

   케라스는 층을 블록처럼 쌓을 수 있다.
   즉, 호환 가능한 층들을 엮어 데이터 변환 파이프 라인을 구성해서 모델을 만든다.

   > 층 호환성 : 각 층이 특정 크기의 입력 텐서만 받고, 특정 크기의 출력 텐서를 반환.\

   

   ```python
   layer = layers.Dense(32, input_shape=(784,)) # 입력 (784,), 출력 (32,)
   model = models.Sequential()
   model.add(layers.Dense(32, input_shape=(784,))
   model.add(layers.Dense(10)))
   ```

   두번째 층에서는 input_shape가 앞선 층의 출력크기로 자동으로 결정됨.

   > 배치 크기는 fit() 메서드에서 정의.
   > LSTM의 경우 현재 셀 상태를 다음 번 배치의 셀 상태 초깃값으로 사용하기 위해 stateful = True.
   > 이 때는 batch_input_shape를 사용해서 배치 크기가 포함된 입력 텐서 크기를 지정해야 함.

   하나의 입력, 하나의 출력 형태 말고도 다양한 네트워크 구조 존재.

   - 가지(branch)가 2개인 네트워크
   - 출력이 여러개
   - 인셉션(Inception) 블록

   네트워크 구조는 가설 공간(hypothesis space)을 정의.

   
   손실함수는 완전히 새로운 연구를 제외하고 대체적으로

   - 2개 클래스 분류 : binary crossentropy
   - 여러개 클래스 분류 : categorical crossentropy
   - 회귀 : Mean Squared Error
   - 시퀀스 학습 문제 : CTC(Connection Temporal Classification)

#### 3-2 케라스

케라스에서 모델을 정의하는 방법은 두 가지.

- Sequential 클래스 : 가장 자주 사용하는 구조인 층을 순서대로 쌓아 올린 네트워크
- 함수형 API : 완전히 임의의 구조를 만들 수 있는 비순환 유향 그래프

먼저 Sequential 클래스를 사용하는 방법.

```python
from keras import models
from keras import layers

network = models.Sequential()
network.add(layers.Dense(512, activation='relu', input_shape=(28*28,)))
network.add(layers.Dense(10, activation='softmax'))
```

같은 모델을 함수형 API로 구성하는 방법.

```python
input_tensor = layers.Input(shape=(784,))
x = layers.Dense(32, activation='relu')(input_tensor)
output_tensor = layers.Dense(10, activation='softmax')(x)

model = models.Model(inputs=input_tensor, outputs=output_tensor)
```

함수형 API를 사용하면 모델이 처리할 데이터 텐서를 만들고 마치 함수처럼 이 텐서에 층을 적용.

#### 3-3. 영화 리뷰 분류: 이진 분류 예제

IMDB(Internet Movie Database)로부터 가져온 리뷰 5만 개로 이루어진 데이터셋 사용.
train_data 25,000, test_data 25,000, 클래스는 긍정, 부정으로 각각 50%의 비율.

여기서 훈련 데이터와 테스트 데이터를 나누는 이유는 모델이 훈련 데이터에 잘 작동한다는 것이 처음 만난 데이터에서도 잘 작동한다는 것을 보장하지 않기 때문. 중요한 것은 새로운 데이터에 대한 모델의 성능이다. 이 데이터는 케라스에서 전처리되어 각 리뷰(단어 시퀀스)가 숫자 시퀀스로 변환되어 저장되어 있음.

1. 데이터 준비

   신경망에 숫자 리스트를 넣을 수 없다. 리스트를 텐서로 바꾸는 두 가지 방법.

   - 같은 길이가 되도록 리스트에 패딩(padding)을 추가하고 (samples, sequence_length) 크기의 정수 텐서로 변환.
     그다음 이 정수 텐서를 다룰 수 있는 층을 신경망의 첫 번째 층으로 사용(Embedding 층 의미)
   - 리스트를 원-핫 인코딩(one-hot encoding)하여 0과 1의 벡터로 변환.그다음 부동 소수 벡터 데이터를 다룰 수 있는 Dense 층을 신경망의 첫 번째 층으로 사용.

   ```python
   import numpy as np
   
   def vectorize_sequences(sequences, dimension=10000):
       results = np.zeros((len(sequences), dimension))
       for i, sequence in enumerate(sequences):
           results[i, sequence] = 1.
       return results
   
   x_train = vectorize_sequences(train_data)
   x_test = vectorize_sequences(test_data)
   ```

2. 신경망 모델 만들기

   입력 데이터가 벡터, 레이블은 스칼라(1 또는 0).
   이런 문제에 잘 작동하는 네트워크 종류는 relu 활성화 함수를 사용한 완전 연결 층을 그냥 쌓은 것.

   Dense 층에 전달한 매개변수는 히든 유닛의 개수.
   Dense 층을 코드로 표현하면

   ```python
   output = relu(dot(W, input) + b)
   ```

   16개의 은닉 유닛이 있다는 것은 가중치 행렬 W의 크기가 (input_dimension, 16)이라는 의미.

   
   Dense 층을 쌓을 때 두 가지 중요한 구조상의 결정이 필요.

   - 얼마나 많은 층을 사용할 것인가?
   - 각 층에 얼마나 많은 은닉 유닛을 둘 것인가?

   4장에서 원리 배울 예정.

   > <b>Q. 활성화 함수가 왜 필요한가?</b>
   >
   > relu와 같은 활성화 함수(또는 비선형성))가 없다면 Dense층은 선형적인 연산인 dot product와 덧셈 2개로 구성된다. 그러므로 이 층은 입력에 대한 선형 변환(Affine transform)만을 학습할 수 있다. 따라서 선형 층을 깊게 쌓아도 여전히 하나의 선형 연산이기 때문에 층을 여러 개로 구성하는 장점이 ㅓㅄ다. 즉, 층을 추가해도 가설 공간이 확장되지 않는다.
   >
   > 가설 공간을 풍부하게 만들어 층을 깊게 만ㄷ느는 장점을 살리기 위해 비선형성 또는 활성화 함수를 추가하는 것.

3. 훈련 검증

   Training와 Validation 데이터에 대한 loss와 acc를 그래프로 그릴 수 있다.
   이 때, 과적합(overfitting)이 일어났다고 판단 된 부분까지 학습을 하는 것이 적절하다.

#### 3-5. 뉴스 기사 분류: 다중 분류 문제

레이블을 인코딩할 때, 정수 텐서로 변환을 한다면 손실 함수도 바꿔야 한다.

```python
y_train = np.array(train_labels)
model.compile(optimizer='rmsprop',
              loss='sparse_categorical_crossentropy',
              metrics=['acc'])
```

sparse_categorical_crossentropy는 수학적으로는 완전히 동일하다.

또한 모델을 구성할 때는 마지막 출력보다 히든 유닛이 많이 적어지면 병목 현상으로 성능이 떨어진다.
많은 정보를 중간층의 저차원 표현 공간으로 압축하는 구조가 되기 때문이다. (ex 46->8->8->46)

#### 3-6. 주택 가격 예측: 회귀 문제

> 회귀와 로지스틱 회귀 알고리즘은 다르다. 로지스틱 회귀는 분류 알고리즘이다.

상이한 스케일을 가진 값을 신경망에 사용하면 Global minimum을 찾아가는 경사 하강법의 경로가 스케일이 큰 특성 위주로 영향을 받게 되는 문제가 생긴다. 이런 데이터를 다룰 때 대표적인 방법은 특성별로 정규화를 하는 것.

또한 metrics에는 다른 것을 사용할 수 있는데, 여기서는 평균 절대 오차(Mean Absolute Error, MAE)를 사용.

- K-fold cross-validation

  데이터의 전체 수가 적을 때는 Validation과 Test에 어떤 데이터가 선택되는지에 따라 검증 점수가 크게 달라질 수 있다. 이렇게 되면 신뢰있는 모델 평가를 할 수 없다. 이럴 때 사용하는 방법이 K-겹 교차 검증이다.

  데이터를 K개의 분할(fold)로 나누고(일반적으로 K는 4 또는 5), K개의 모델을 각각 만들어 K-1개의 분할에서 훈련하고 나머지 분할에서 평가하는 방법. 모델의 검증 점수는 K개의 검증 점수 평균이다.

![kfold](https://user-images.githubusercontent.com/27988544/65207902-51ae3400-dace-11e9-9420-5ed662e12279.png)

```python
import numpy as np

k = 4
num_val_samples = len(train_data) // k
num_epochs = 100
all_scores = []
for i in range(k):
    print('처리중인 폴드 #', i)
    val_data = train_data[i * num_val_samples: (i+1) * num_val_samples]
    val_targets = train_targets[i * num_val_samples: (i+1) * num_val_samples]
    
    partial_train_data = np.concatenate(
        [train_data[:i * num_val_samples],
         train_data[(i+1) * num_val_samples:]],
         axis=0)
    
    model = build_model()
    model.fit(partial_train_data, partial_train_targets,  # verbose=0 이면 훈련과정 출력 X
              epochs=num_epochs, batch_size=1, verbose=0)
    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)
    all_scores.append(val_mae)
```

결과를 그래프로 확인할 때, 범위가 크고 변동이 심할 경우 다음과 같이하면 좋다.

- 곡선의 다른 부분과 스케일의 차이가 큰 첫 10개의 데이터 포인트를 제외.

- 부드러운 곡선을 얻기 위해 각 데이터 포인트를 이전 포인트의 지수 이동 평균(exponential moving average)으로 대체.

  > 지수 이동 편균은 시계열 데이터를 부드럽게 만드는 기법 중 하나.
  >
  > ```python
  > def smmoth_curve(points, factor=0.9):
  >     smmothed_points = []
  >     for point in points:
  >         if smoothed_points:
  >             previous = smoothed_points[-1]
  >             smoothed_points.append(previous * factor + point * (1 - factor)) # 이 부분!
  >         else:
  >             smoothed_points.append(point)
  >     
  >     return smoothed_points
  > ```

### &#128218; 4장. 머신러닝의 기본 요소

#### 4-1. 머신러닝의 네 가지 분류

1. 지도 학습 (supervised learning)

   샘플 데이터가 주어지면 타깃에 입력 데이터를 매핑하는 방법을 학습.

   - 시퀀스 생성 : 사진이 주어지면 이를 설명한느 캡션을 생성. 일련의 분류 문제로 재구성 가능
   - 구문 트리(syntax tree) 예측 : 문장이 주어지면 분해된 구문 트리를 예측
   - 물체 감지(object detection) : 사진이 주어지면 특정 물체 주위에 경계 상자(bounding box)를 그린다. 이는 많은 경계 상자 후보가 주어졌을 때 각 상자의 내용을 분류하는 문제로 표현되거나, 경계 상자의 좌표를 벡터 회귀로 예측하는 회귀와 분류가 결합된 문제로도 표현 가능.
   - 이미지 분할(image segmentation) : 사진이 주어졌을 때 픽셀 단위로 특정 물체에 마스킹을 한다.

2. 비지도 학습 (unsupervised learning)

   어떤 타깃도 사용하지 않고 입력 데이터에 대한 흥미로운 변환을 찾는 학습. 데이터 시각화, 데이터 압축, 데이터의 노이즈 제거 또는 데이터에 있는 상관관계를 더 잘 이해하기 위해 사용. 비지도 학습은 데이터 분석에서 빼놓을 수 없는 요소이며, 종종 지도학습 문제를 풀기 전에 데이터셋을 잘 이해하기 위해 필수적으로 거치는 단계.

   - 차원 축소(dimensionoality reduction)
   - 군집(clustering)

3. 자기 지도 학습 (self-supervised learning)

   지도 학습의 특별한 경우지만 별도의 범주로 할 만큼 충분히 다른 학습 방법. 자기 지도학습은 사람이 만든 레이블을 사용하지 않는다. 즉 학습 과정에 사람이 개입하지 않는 지도 학습이다. 레이블은 여전히 필요하지만 보통 경험적인 알고리즘(heuristic algorithm)을 사용해서 입력 데이터로부터 생성한다.

   - <b>오토인코더(autoencoder)</b> : 이 모델에서의 타깃은 수정하지 않은 원본 입력. 같은 방식으로 지난 프레임이 주어졌을 때 비디오의 다음 프레임을 예측하는 것이나, 이전 단어가 주어졌을 때 다음 단어를 예측하는 것이 자기 지도 학습이 대표적인 예시 (이 경우에는 미래의 입력 데이터로부터 지도되기 때문에 시간에 따른 지도 학습(temporally supervised learning)이다).

4. 강화 학습 (reinforcement learning)

   구글 딥마인드가 아타리(Atari) 게임 플레이를 학습하는데 성공적으로 적용하면서 주목을 받기 시작한 학습 방법. 강화 학습에서 <b>에이전트(agent)</b>는 환경에 대한 정보를 받아 보상을 최대화하는 행동을 선택하도록 학습됨.

#### 4-2. 머신러닝 모델 평가

머신러닝의 목표는 처음 본 데이터에서 잘 작동하는 <b>일반화</b>된 모델을 얻는 것. 여기에서 Overfitting은 주요 작애물. 관측할 수 있는 것만 제어할 수 있으므로 모델의 일반화 성능에 대한 신뢰할 수 있는 측정 방법이 중요하다.

1. 훈련, 검증, 테스트 세트

   모델 평가의 핵심은 데이터를 항상 훈련, 검증, 테스트 3개의 세트로 나누는 것. 훈련 세트에서 모델을 훈련하고 검증 세트에서 모델을 평가. 모델 출시 준비가 되면 테스트 세트에서 최종적으로 딱 한 번 모델을 테스트.

   훈련 세트와 테스트 세트 2개만을 사용하면 되지 않을까? 라는 의문이 생길 수 있다.
   이렇게 하지 않는 이유는 모델을 개발할 때 항상 모델의 설정을 튜닝하기 때문이다. 예를 들어 층의 수나, 유닛 수 등의 하이퍼파라미터를 조정해서 검증 세트를 통해 모델의 성능을 평가해야하기 때문이다.

   본질적으로 튜닝 작업도 어떤 파라미터 공간에서 좋은 설정을 찾는 학습이라고 생각할 수 있다. 결국 검증 세트의 성능을 기반으로 모델의 설정을 튜닝하면 검증 세트로 모델을 직접 훈련하지 않더라도 빠르게 검증 세트에 과적합될 수 있다.
   
   이 현상의 핵심은 <b>정보 누설(information leak)</b> 개념에 있다. 검증 세트의 모델 성능을 보고 하이퍼파라미터를 조정할 때마다 검증 데이터에 관한 정보가 모델로 새는 것으로 볼 수 있다. 한 번은 매우 적은 양이 노출되지만, 딥러닝은 반복적으로 학습하기 때문에 검증 세트에 대해 모델을 많이 노출시키기 된다.
   
   결국 검증 데이터에 맞추어 최적화했기 때문에 완전히 새로운 데이터에 대한 성능 측정이 필요하다. 그래서 데이터 세트를 3개로 나눠야 한다. 이렇게 데이터를 나누는 것은 간단해 보일 수 있지만 데이터가 적을 때는 몇가지 고급 기법을 사용하면 도움이 된다. 대표적인 세 가지 평가 방법은 다음과 같다.
   
   - 홀드아웃 검증 (hold-out validation)
   
     테스트 세트와 트레이닝 세트를 먼저 나누고, 트레이닝 세트에서 홀드아웃 검증 세트를 분할한다.
   
     > 사이킷런의 train_test_split()함수를 사용하여 훈련, 검증, 테스트 세트로 나눌 수 있음. 
   
   - K-겹 교차 검증 (K-fold cross-validation)
   
     데이터를 동일한 크기를 가진 K개 분할로 나눈다. 각 분할 i에 대해 남은 K-1개의 분할로 모델을 훈련하고 분할 i에서 모델을 평가한다. 최종 점수는 K개의 점수의 평균이다. 이 방법은 모델의 성능이 데이터 분할에 따라 편차가 클 때 도움이 된다.
   
     > 사이킷런의 cross_validate() 함수를 사용해서 나눌 수 있음.
     > 이 함수를 사용하려면 케라스 모델을 사이킷런과 호환되도록 KerasClassifier나 KerasRegressor 클래스로 모델을 감싸야 한다.
   
   - 셔플링 (shuffling)을 사용한 반복 K-겹 교차 검증 (iterated K-fold cross-validation)
   
     비교적 가용 데이터가 적고 정확하게 모델을 평가하고자 할 때 사용. 캐글에서 많이 사용. K-겹 교차 검증을 여러 번 적용하되 K개의 분할로 나누기 전에 매번 데이터를 무작위로 섞는다. 이 방법은 P X K개(P는 반복 횟수)의 모델을 훈련하고 평가하므로 비용이 크다.
   
     > 사이킷런의 RepeatedKFold(회귀)와 RepeatedStaratifiedKFold(분류) 클래스를 cross_validate() 함수에 적용하여 구현.
   
2. 데이터에 대한 평가 방법 선택시 주의사항

   - 대표성 있는 데이터 :훈련 세트와 테스트 세트가 주어진 데이터에 대한 대표성이 있어야 한다.
   - 시간의 방향 : 과거로부터 미래 예측을 하는게 목적이라면 데이터를 분할하기 전에 무작위로 섞으면 안된다.
   - 데이터 중복 : 한 데이터셋에 데이터 포인트가 두 번 등장하면, 데이터를 섞고 훈련 세트와 검증 세트로 나누었을 때 각각에 들어갈 수 있다. 훈련 세트와 검증 세트가 중복되지 않는지 확인하세요.

#### 4-3. 데이터 전처리, 특성 공학, 특성 학습

많은 데이터 전처리와 특성 공학 기법은 특정 도메인에 종속적이다.

1. 신경망을 위한 데이터 전처리

   데이터 전처리 목적은 주어진 원본 데이터를 신경망에 적용하기 쉽도록 만드는 것.

   - 벡터화 (vectorization)

     신경망에서 모든 입력과 타깃은 부동 소수(특정 경우에는 정수) 데이터로 이루어진 텐서여야 한다.
     이 단계를 <b>데이터 벡터화(data vectorization)</b>라고 한다.

   - 정규화 (normalization)

     숫자 이미지 분류에서 이미지 데이터를 그레이스케일 인코딩인 0!255 사이의 정수로 인코딩한다. 이 데이터를 네트워크에 주입 전에 float32 타입으로 변경하고 255로 나누어서 최종적으로 0~1 사이의 부동 소수값으로 만든다.또는 어떤 값이 들어오더라도 특성 값의 범위를 평균이 0이고 표준편차가 1이 되도록하는 방법도 가능하다.

     즉, 모든 특성이 대체로 비슷한 범위를 가져야 한다.

   - 누락된 값 다루기

     일반적으로 신경망에서 0이 사전에 정의된 의미있는 값이 아니라면 누락된 값을 0으로 입력해도 괜찮다.
     경우에 따라 누락된 값을 고의적으로 만들어 학습시키는 것도 필요할 수 있다.

2. 특성 공학

   데이터와 머신러닝에 관한 지식을 사용하는 단계. 모델에 데이터를 주입하기 전에 하드코딩된 변환을 적용하여 알고리즘이 더 잘 수행되도록 만들어 준다. 다시 말해, 주입되는 데이터 자체를 고차원에서 저차원적인 데이터로 변환시키는 것. 이를 통해 기존의 문제보다 훨씬 단순한 문제를 푸는 것으로 바꿀 수 있다.

   EX) MNIST 숫자 이미지 분류 문제

   - 숫자 이미지에 있는 동심원의 수
   - 이미지에 있는 숫자의 높이
   - 픽셀 값의 히스토그램

   딥러닝은 대부분 특성 공학이 필요하지 않다. 자동으로 원본 데이터에서 유용한 특성을 추출할 수 있기 때문이다. 하지만 딥러닝 말고도 특성 공학을 사용하면 좋은 점!

   - 좋은 특성은 적은 자원을 사용해서 문제를 쉽게 풀어낼 수 있다. 예를 들어 시계 바늘을 읽는 문제를 이미지가 아니라 좌표값으로 사용한다면 신경망을 사용하지 않아도 된다.
   - 더 적은 데이터로 문제를 풀 수 있다. 샘플의 개수가 적다면 특성에 있는 정보가 매우 중요하기 때문에 특성 공학이 더 적합할 수 있다.

#### 4-4. Overfitting, Underfitting

모델을 학습하다보면 홀드아웃 데이터(Validation)에서 모델의 성능이 어느 시점에서 최고치에 다다랐다가 감소되기 시작한다. 즉 모델이 훈련 데이터에 Overfitting되기 시작한 것.

반대로 Overfitting은 모델의 성능이 계속 발전될 여지가 있는 상황. 즉, 관련 특성을 모두 학습하지 못한 상태.

모델이 관련성이 없고 좋지 못한 패턴을 훈련 데이터에서 학습하지 못하도록 하려면 <b>가장 좋은 방법은 더 많은 훈련 데이터를 모으는 것</b>. 데이터를 더 모으는 것이 불가능할 때 차선책은 모델이 수용할 수 있는 정보의 양을 조절하거나 저장할 수 있는 정보에 제약을 가하는 것. 네트워크가 적은 수의 패턴만 기억할 수 있다면 최적화 과정에서 가장 중요한 패턴에 집중하게 될 것이기 때문이다.

이러한 방식으로 Overfitting을 피하는 처리 과정을 <b>규제(regularization)</b>라고 함.

1. 네트워크 크기 축소

   Overfitting을 방지하는 가장 단순한 방법. 모델의 파라미터 수(layer 수, unit 수)를 줄이는 것.
   이 때 중요한 것은 네트워크가 기억 용량이 압축된 핵심 표현만을 가질 수 있는 최소 크기를 찾아야 한다는 것이다.

   적절한 모델 크기를 찾는 일반적인 흐름은 비교적 적은 수의 층과 파라미터로 시작, 검증 데이터의 손실이 감소되기 시작할 때 까지 층이나 유닛의 수를 늘리는 방식.

2. 가중치 규제 추가

   > <b>오캄의 면도날(Occam's razor)</b>
   > 어떤 것에 대한 두 가지의 설명이 있다면 더 적은 가정이 필요한 간단한 설명이 옳을 것.

   간단한 모델이 복잡한 모델보다 Overfitting될 가능성이 적다. 
   (여기서 간단한 모델은 파라미터 값 분포의 엔트로피가 작은 모델, 혹은 적은 수의 파라미터를 가진 모델.)

   그러므로 Overfitting을 방지하기 위한 일반적인 방법은 네트워크의 복잡도에 제한을 두어 가중치가 작은 값을 가지도록 강제한다. 이를 통해 가중치의 분포가 더 균일하게 만들 수 있는데, 이를 <b>가중치 규제(weight regularization)</b>라고 하며, 네트워크의 손실 함수에 큰 가중치에 연관된 비용을 추가한다. 이에 두 가지 형태의 비용이 있다.

   - L1 규제 : 가중치의 절댓값에 비례하는 비용이 추가됨(가중치의 L1 노름(norm)).

   - L2 규제 : 가중치의 제곱에 비례하는 비용이 추가됨(가중치의 L2 노름). L2 규제는 신경망에서 <b>가중치 감쇠(weight decay)</b>라고도 부름. 수학적으로 L2 규제와 동일함.

     + L2 노름은 유클리디안 노름(Euclidean norm)이라고도 부름.

     ```python
     from keras import regularizers
     
     model = models.Sequential()
     # l2(0.001)은 가중치 행렬의 모든 원소를 제곱하고
     # 0.001을 곱해서 네트워크의 전체 손실에 더해진다는 의미.
     # 이 페널티(penalty) 항은 훈련할 때만 추가된다.
     model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),
                            activation='relu', input_shape=(10000,)))
     model.add(layers.Dense(16, kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),
                            activation='relu'))
     ```

     > <b>+ 추가 정보</b>
     >
     > L2 규제는 가중치 값을 작게 만들지만 완전히 0이 되지는 않는다. L1 규제는 일부 가중치 값을 완전히 0으로 만들 수 있다. L1 규제와 L2 규제를 함께 쓰는 방식을 <b>엘라스틱넷(ElasticNet)이라고 부른다.

   - 드롭아웃 추가

     신경망을 위해 사용되는 규제 기법 중에서 가장 효과적이고 널리 사용되는 방법 중 하나. 훈련하는 동안 무작위로 층의 일부 출력 특성을 제외시키는 기법. 테스트 단계에서는 어떤 유닛도 드롭아웃되지는 않지만, 층의 출력 결과를 드롭아웃 비율에 비례해서 줄여준다.

     핵심 아이디어는 층의 출력 값에 노이즈를 추가하여 중요하지 않은 우연한 패턴을 깨드리는 것.

     > +배경 지식
     > <a href="http://mng.bz/XrsS">AMA: We are the Google Brain team. We'd love to answer your questions about machine learning</a>

     

     ```python
     model.add(layers.Dropout(0.5))
     ```

