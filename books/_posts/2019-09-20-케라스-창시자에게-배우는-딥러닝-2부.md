# 2019-09-18-케라스 창시자에게 배우는 딥러닝 -2부-

2019.09.20 시작

## &#128214; 2부

### &#128218; 5장. 컴퓨터 비전을 위한 딥러닝

#### 5-1. CNN

1. MNIST 데이터를 사용한 심플한 예제

   ```python
   #########
   # model.py
   from keras import layers
   from keras import models
   
   model = models.Sequential()
   model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
   model.add(layers.MaxPooling2D((2, 2)))
   model.add(layers.Conv2D(64, (3, 3), activation='relu'))
   model.add(layers.MaxPooling2D((2, 2)))
   model.add(layers.Conv2D(64, (3, 3), activation='relu'))
   
   # 분류기
   model.add(layers.Flatten())
   model.add(layers.Dense(64, activation='relu'))
   model.add(layers.Dense(10, activation='softmax'))
   ```

   ```python
   ##########
   # train.py
   from keras.datasets import mnist
   from keras.utils import to_categorical
   import model
   
   (train_images, train_labels), (test_images, test_labels) = mnist.load_data()
   train_images = train_images.reshape((60000, 28, 28, 1))
   train_images = train_imaes.astype('float32') / 255
   
   test_images = test_imges.reshape((10000, 28, 28, 1))
   test_images = test_iamges.astype('float32') / 255
   
   train_labels = to_categorical(train_lbaels)
   test_labels = to_categorical(test_labels)
   
   model.compile(optimizer='rmsprop',
                 loss='categorical_crossentropy',
                 metrics=['accuracy'])
   model.fit(train_images, train_labels, epochs=5, batch_size=64)
   ```

   - ConvNet은 (image_height, image_width, image_channels) 크기의 입력 텐서를 사용한다.
   - 높이와 너비 차원은 네트워크가 깊어질수록 작아지는 경향이 있다.
   - Dense 층에 넣기 위해서는 3D 출력은 1D 텐서로 펼쳐야 한다.

2. 합성곱 연산

   - Dense 층은 입력의 전역 패턴을 학습
   - Conv 층은 지역 패턴을 학습 (이미지는 에지(edge), 질감(texture) 등 지역 패턴으로 분해될 수 있기 때문)

   이러한 특징 때문에 ConvNet는 다음과 같은 성질이 있다.

   - 학습된 패턴은 평행 이동 불변성(translation invariant)을 가진다. 패턴을 학습하면 위치는 상관이 없다. 이로인해 적은 수의 훈렴 샘플을 사용해서 일반화 능력을 가질 수 있도록 학습 가능
   - 패턴의 공간적 계층 구조를 학습할 수 있다. 작은 지역 패턴부터 큰 영역 패턴까지. 그 이유는 층을 통과하면서 크기가 작아지는 반면, 윈도우 사이즈는 그대로이기 때문에 더 넓은 영역에서 패턴을 찾게 된다.

3. 패딩 (padding)

   5x5 특성 맵에서 3x3 윈도우로 합성곱을 할 수 있는 부분은 3x3 크기를 가진다.
   따라서 입력으로 들어온 크기보다 결과가 작아지게 된다.

   입력과 동일한 크기를 가진 출력은 얻기 위해서 패딩을 사용한다. 패딩은 입력 특성 맵의 가장자리에 윈도우크기 $$W_{size}$$ /2 만큼 행,열을 0으로 추가한다.

   > keras에서 Conv2D에 padding="valid | same"

4. 스트라이드 (stride)

   출력 크기에 영향을 미치는 한 가지 요소. Conv는 윈도우가 모든 영역을 움직이면서 훑는 것. 이 때 두 번의 연속ㅈ거인 윈도우 사이의 거리를 스트라이드라는 파라미터로 설정 가능.

5. 최대 풀링 연산

   MaxPooling2D는 강제적으로 특성 맵을 Down Sampling하는 것.
   최대 풀링은 윈도우에 맞는 패치(patch)를 추출하고 각 채널별로 최댓값을 출력.

   최대 풀링은 사용하는 이유는

   - 처리할 특성 맵의 가중치 개수를 줄이기 위해
   - Conv 층에 사용되는 윈도우가 더 넓은 영역을 커버할 수 있도록

#### 5-2. 소규모 데이터셋에서 밑바닥부터 컨브넷 훈련하기

문제를 해결하기 위해 수집한 데이터의 수가 적을 경우는 매우 많다.
그리고 그 데이터를 훈련, 테스트, 검증 세 분류로 나눠서 사용해야 한다.

데이터 수에 관련된 문제를 해결하기 위해서 5-2절에서는 <b>데이터 증식(data augmentation)</b>을 사용한다.

1. 작은 데이터셋 문제에서 딥러닝의 타당성

   딥러닝의 근본적인 특징은 훈련 데이터에서 특성 공학의 수작업 없이 흥미로운 특성을 찾을 수 있다는 것이고, 이는 훈련 샘플이 많아야 가능하다. 특히 이미지처럼 매우 고차원인 문제에서는 특히 그렇다.

   하지만 훈련 샘플이 많다는 것은 네트워크의 크기와 깊이에 상대적이다. ConvNet은 지역적이고 평행 이동으로 변하지 않는 특성을 학습하기 때문에 지각에 관한 문제에서 매우 효율적으로 데이터를 사용한다. 따라서 모델이 작고 규제가 잘 되어 있다면 충분히 성능을 낼 수 있다.

   그리고 딥러닝 모델은 매우 다목적이다. 다른 모델을 조금만 변경해서 다른 문제에 재사용할 수 있고, 특히 컴퓨터 비전에서는 ImageNet 데이터로 사전 훈련된 모델들이 존재해서 매우 적은 데이터에서 강력한 비전 모델을 만드는데 사용할 수 있다.

2. 이미지 전처리

   keras.preprocessing.image의 ImageDataGenerator을 이용해서 픽셀 값을 0~1로 조정하고, flow_from_directory() 메소드를 이용해서 해당 디렉토리로 부터 데이터를 가져온다.

   ```python
   from keras.preprocessing.image import ImageDataGenerator
   
   train_datagen = ImageDataGenerator(rescale=1./255)
   test_datagen = ImageDataGenerator(rescale=1./255)
   
   train_generator = train_datagen.flow_from_directory(
   		train_dir,		# 사전에 경로 설정
   		target_size=(150, 150),
   		batch_size=20,
   		class_mode='binary')
   
   validation_generator = test_datagen.flow_from_directory(
   		validation_dir,  # 사전에 경로 설정
   		target_size=(150, 150),
   		batch_size=20,
   		class_mode='binary')  # 다중 분류 : categorical, sparse, 이진 분류 : binary
   						    # categorical은 원-핫 인코딩된 2차원 배열 반환
       						# sparse는 정수 레이블을 담은 1차원 배열 반환
   ```

3. 모델 훈련

   ```python
   history = model.fit_generator(train_generator,
                                 steps_per_epoch=100,
                                 epochs=30,
                                 validation_data=validation_generator,
                                 validation_steps=50)
   model.save('model.h5')
   ```

4. 시각화

   ```python
   import matplotlib.pyplot as plt
   
   acc = history.history['acc']
   val_acc = history.history['val_acc']
   loss = history.history['loss']
   val_loss = history.history['val_loss']
   
   epochs = range(1, len(acc) + 1)
   
   plt.plot(epochs, acc, 'bo', label='Training acc')
   plt.plot(epochs, val_acc, 'b', label='Validation acc')
   plt.title('Training and validation accuracy')
   plt.legend()
   
   plt.figure()
   
   plt.plot(epochs, loss, 'bo', label='Training loss')
   plt.plot(epochs, val_loss, 'b', label='Validation loss')
   plt.title('Training and validation loss')
   plt.legend()
   
   plt.figure()
   ```

5. data augmentation

   데이터에 랜덤한 변환을 적용하여 샘플을 늘리는 기법.

   ```python
   datagen = ImageDataGenerator(rotation_range=20,			# 회전
                                width_shift_range=0.1,		# 평행 이동
                                height_shift_range=0.1,	
                                shear_range=0.1,			# shearing transformation
                                zoom_range=0.1,			# 확대
                                horizontal_flip=True,		# 수평 전환
                                fill_mode='nearest')		# 회전이나 가로/세로 이동으로 인해
   													# 새롭게 생성해야할 픽셀 채울 전략
       												# nearest, constant, reflect, wrap
   ```

6. regularization

   Dropout을 Dense 층 앞에 추가하면 더 성능이 좋아질 가능성이 있음. Overfitting 방지.

#### 5-3. 사전 훈련된 ConvNet 사용

pretrained network를 사용하는 두 가지 방법. <b>특성 추출(feature extraction), 미세 조정(fine tuneing)</b>

1. 특성 추출

   사전 학습된 네트워크의 표현을 사용해서 새로운 샘플에서 특성을 뽑아내는 것. 이런 특성을 사용해서 새로운 분류기를 처음부터 훈련한다. 즉, 사전 학습된 네트워크의 Conv 층만 재사용한다.

   특정 Conv 층에서 추출한 표현의 일반성과 재사용성 수준은 층의 깊이에 따라 다르다. 모델의 하위 층은 (에지, 색, 질감 등) 지역적이고 매우 일반적인 특성 맵을 추출하고, 상위 층은 ('강아지 눈'이나 '고양이 귀' 등) 추상적인 개념을 추출한다. 따라서 새로운 데이터셋이 원본 모델이 훈련한 데이터셋과 많이 다르다면 모델의 하위 층 몇개만 특성 추출에 재사용하는 것이 좋다.

   keras.applications 모듈에서 임포트 할 수 있는 이미지 분류 모델

   - Xception		: "Xception: Deep Learning with Depthwise Separable Convolutions," arXiv (2016)
   - Inception V3  : "Rethinking the Inception Architecture for Computer Vision," arXiv (2015)
   - ResNet50      : "Deep Residual Learning for Image Recognition," arXiv (2015)
   - VGG16          : "Very Deep Convolutional Network for Large-Scale Image Recognition," arXiv (2014)
   - MobileNet      : "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications," arXiv (2017)
   - DenseNet      : "Densely Connected Convolutional Networks," arXiv (2016)
   - NASNet         : "Learning Transferable Architectures for Scalable Image Recognition," arXiv (2017)

   ```python
   from keras.applications import VGG16
   
   conv_base = VGG16(weights='imagenet',
                     include_top=False,
                     input_shape=(150, 150, 3))
   ```

   위의 코드에서 weights는 모델을 초기화할 가중치 체크포인트(checkpoint)를 지정. include_top은 네트워크의 최상위 완전 연결 분류기 포함 여부를 결정.

   사전 학습 모델을 이용해서 새로운 모델을 학습시키는 방법은 두 가지가 있다.

   1. 비용이 적은 방법 : 기존 모델에 새로운 데이터셋을 입력으로 실행해서 나온 출력은 NumPy 배열로 저장. 그 다음 독립된 분류기에 입력으로 넣어 학습. 이 방법은 전체 과정에서 가장 비용이 많이 드는 Conv 연산이 모든 입력 이미지에 대해 한 번만 실행하면 되므로 빠르고 비용이 적다. 하지만 data augmentation을 사용할 수 없다.

   2. end-to-end 방법 : 준비한 모델위에 새로운 분류기를 쌓아서 학습. 비용이 많이 든다.

      ```python
      from keras import models
      from keras import layers
      
      model = models.Sequential()
      mode.add(conv_base)
      model.add(layers.Flatten())
      model.add(layers.Dense(256, activation='relu'))
      model.add(layers.Dense(1, activation='sigmoid'))
      ```

      이 방법을 사용할 때 가장 중요한 것은 모델을 컴파일하고 훈련하기 전에 Conv_base를 동결하는 것이다. 층을 동결한다는 것은 훈련하는 동안 가중치가 업데이트되지 않도록 막는다는 뜻이며, 사전에 학습된 표현이 수정되지 않도록 하는 방법이다.

      keras에서는 trainable 속성을 False로 설정하면 된다.

      ```python
      conv_base.trainable = False
      ```

2. 미세 조정 (fine tuning)

   특성 추출에 사용했던 동결 모델의 상위 층 몇개를 동결에서 해제하고 모델에 새로 추가한 층과 함께 훈련하는 것.
   주어진 문제에 조금 더 밀접하게 재사용 모델의 표현을 일부 조정하기 때문에 미세 조정이라고 함.

   이 때 분류기를 미리 훈련하지 않으면 너무 큰 오차 신호가 전파되어 사전에 학습한 표현들이 망가질 수 있다. 그래서 학습 방법은 다음과 같다.

   - 사전에 훈련된 기반 네트워크 위에 새로운 네트워크를 추가
   - 기반 네트워크를 동결
   - 새로 추가한 네트워크 훈련
   - 기반 네트워크에서 일부 층의 동결을 해제
   - 동결을 해제한 층과 새로 추가한 층을 함께 훈련

   ```python
   conv_base.summary() # summary 메소드를 통해 기본 모델의 구성 layer을 확인할 수 있음.
   
   conv_base.trainable = True
   
   set_trainable = False
   for layer in conv_base.layers:
       if layer.name == 'block5_conv1':
           set_trainable = True
           
       if set_trainable:
           layer.trainable = True
       else:
           layer.trainable = False
   ```

> <b>Q. 손실이 감소되지 않았는데 어떻게 정확도가 안정되거나 향상될 수 있을까?</b>
>
> 손실은 평균이다. 하지만 정확도에 영향을 미치는 것은 손실 값의 분포이지 평균이 아니다. 정확도는 모델이 예측한 클래스 확률이 어떤 임계 값을 넘었는지에 대한 결과이다. 따라서 모델이 더 향상되더라도 평균 손실에 반영되지 않을 수 있다.

#### 5-4. ConvNet 학습 시각화

- ConvNet 중간층의 출력
- ConvNet 필터를 시각화
- 클래스 활성화에 대한 히트맵을 이미지에 시각화

관련 정리한 내용은 <a href="../../study/_posts/2019-09-20-MACHINE_LEARNING-ConvNet-학습-시각화.md">MACHINE_LEARNING - ConvNet 학습 시각화</a>에서 참고.

