# 2019-09-18-케라스 창시자에게 배우는 딥러닝 -2부-

2019.09.20 시작

## &#128214; 2부

### &#128218; 5장. 컴퓨터 비전을 위한 딥러닝

#### 5-1. CNN

1. MNIST 데이터를 사용한 심플한 예제

   ```python
   #########
   # model.py
   from keras import layers
   from keras import models
   
   model = models.Sequential()
   model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
   model.add(layers.MaxPooling2D((2, 2)))
   model.add(layers.Conv2D(64, (3, 3), activation='relu'))
   model.add(layers.MaxPooling2D((2, 2)))
   model.add(layers.Conv2D(64, (3, 3), activation='relu'))
   
   # 분류기
   model.add(layers.Flatten())
   model.add(layers.Dense(64, activation='relu'))
   model.add(layers.Dense(10, activation='softmax'))
   ```

   ```python
   ##########
   # train.py
   from keras.datasets import mnist
   from keras.utils import to_categorical
   import model
   
   (train_images, train_labels), (test_images, test_labels) = mnist.load_data()
   train_images = train_images.reshape((60000, 28, 28, 1))
   train_images = train_imaes.astype('float32') / 255
   
   test_images = test_imges.reshape((10000, 28, 28, 1))
   test_images = test_iamges.astype('float32') / 255
   
   train_labels = to_categorical(train_lbaels)
   test_labels = to_categorical(test_labels)
   
   model.compile(optimizer='rmsprop',
                 loss='categorical_crossentropy',
                 metrics=['accuracy'])
   model.fit(train_images, train_labels, epochs=5, batch_size=64)
   ```

   - ConvNet은 (image_height, image_width, image_channels) 크기의 입력 텐서를 사용한다.
   - 높이와 너비 차원은 네트워크가 깊어질수록 작아지는 경향이 있다.
   - Dense 층에 넣기 위해서는 3D 출력은 1D 텐서로 펼쳐야 한다.

2. 합성곱 연산

   - Dense 층은 입력의 전역 패턴을 학습
   - Conv 층은 지역 패턴을 학습 (이미지는 에지(edge), 질감(texture) 등 지역 패턴으로 분해될 수 있기 때문)

   이러한 특징 때문에 ConvNet는 다음과 같은 성질이 있다.

   - 학습된 패턴은 평행 이동 불변성(translation invariant)을 가진다. 패턴을 학습하면 위치는 상관이 없다. 이로인해 적은 수의 훈렴 샘플을 사용해서 일반화 능력을 가질 수 있도록 학습 가능
   - 패턴의 공간적 계층 구조를 학습할 수 있다. 작은 지역 패턴부터 큰 영역 패턴까지. 그 이유는 층을 통과하면서 크기가 작아지는 반면, 윈도우 사이즈는 그대로이기 때문에 더 넓은 영역에서 패턴을 찾게 된다.

3. 패딩 (padding)

   5x5 특성 맵에서 3x3 윈도우로 합성곱을 할 수 있는 부분은 3x3 크기를 가진다.
   따라서 입력으로 들어온 크기보다 결과가 작아지게 된다.

   입력과 동일한 크기를 가진 출력은 얻기 위해서 패딩을 사용한다. 패딩은 입력 특성 맵의 가장자리에 윈도우크기 $$W_{size}$$ /2 만큼 행,열을 0으로 추가한다.

   > keras에서 Conv2D에 padding="valid | same"

   

4. 스트라이드 (stride)

   출력 크기에 영향을 미치는 한 가지 요소. Conv는 윈도우가 모든 영역을 움직이면서 훑는 것. 이 때 두 번의 연속적인 윈도우 사이의 거리를 스트라이드라는 파라미터로 설정 가능.

5. 최대 풀링 연산

   MaxPooling2D는 강제적으로 특성 맵을 Down Sampling하는 것.
   최대 풀링은 윈도우에 맞는 패치(patch)를 추출하고 각 채널별로 최댓값을 출력.

   최대 풀링은 사용하는 이유는

   - 처리할 특성 맵의 가중치 개수를 줄이기 위해
   - Conv 층에 사용되는 윈도우가 더 넓은 영역을 커버할 수 있도록

#### 5-2. 소규모 데이터셋에서 밑바닥부터 컨브넷 훈련하기

문제를 해결하기 위해 수집한 데이터의 수가 적을 경우는 매우 많다.
그리고 그 데이터를 훈련, 테스트, 검증 세 분류로 나눠서 사용해야 한다.

데이터 수에 관련된 문제를 해결하기 위해서 5-2절에서는 <b>데이터 증식(data augmentation)</b>을 사용한다.

1. 작은 데이터셋 문제에서 딥러닝의 타당성

   딥러닝의 근본적인 특징은 훈련 데이터에서 특성 공학의 수작업 없이 흥미로운 특성을 찾을 수 있다는 것이고, 이는 훈련 샘플이 많아야 가능하다. 특히 이미지처럼 매우 고차원인 문제에서는 특히 그렇다.

   하지만 훈련 샘플이 많다는 것은 네트워크의 크기와 깊이에 상대적이다. ConvNet은 지역적이고 평행 이동으로 변하지 않는 특성을 학습하기 때문에 지각에 관한 문제에서 매우 효율적으로 데이터를 사용한다. 따라서 모델이 작고 규제가 잘 되어 있다면 충분히 성능을 낼 수 있다.

   그리고 딥러닝 모델은 매우 다목적이다. 다른 모델을 조금만 변경해서 다른 문제에 재사용할 수 있고, 특히 컴퓨터 비전에서는 ImageNet 데이터로 사전 훈련된 모델들이 존재해서 매우 적은 데이터에서 강력한 비전 모델을 만드는데 사용할 수 있다.

2. 이미지 전처리

   keras.preprocessing.image의 ImageDataGenerator을 이용해서 픽셀 값을 0~1로 조정하고, flow_from_directory() 메소드를 이용해서 해당 디렉토리로 부터 데이터를 가져온다.

   ```python
   from keras.preprocessing.image import ImageDataGenerator
   
   train_datagen = ImageDataGenerator(rescale=1./255)
   test_datagen = ImageDataGenerator(rescale=1./255)
   
   train_generator = train_datagen.flow_from_directory(
   		train_dir,		# 사전에 경로 설정
   		target_size=(150, 150),
   		batch_size=20,
   		class_mode='binary')
   
   validation_generator = test_datagen.flow_from_directory(
   		validation_dir,  # 사전에 경로 설정
   		target_size=(150, 150),
   		batch_size=20,
   		class_mode='binary')  # 다중 분류 : categorical, sparse, 이진 분류 : binary
                                  # categorical은 원-핫 인코딩된 2차원 배열 반환
                                  # sparse는 정수 레이블을 담은 1차원 배열 반환
   ```

3. 모델 훈련

   ```python
   history = model.fit_generator(train_generator,
                                 steps_per_epoch=100,
                                 epochs=30,
                                 validation_data=validation_generator,
                                 validation_steps=50)
   model.save('model.h5')
   ```

4. 시각화

   ```python
   import matplotlib.pyplot as plt
   
   acc = history.history['acc']
   val_acc = history.history['val_acc']
   loss = history.history['loss']
   val_loss = history.history['val_loss']
   
   epochs = range(1, len(acc) + 1)
   
   plt.plot(epochs, acc, 'bo', label='Training acc')
   plt.plot(epochs, val_acc, 'b', label='Validation acc')
   plt.title('Training and validation accuracy')
   plt.legend()
   
   plt.figure()
   
   plt.plot(epochs, loss, 'bo', label='Training loss')
   plt.plot(epochs, val_loss, 'b', label='Validation loss')
   plt.title('Training and validation loss')
   plt.legend()
   
   plt.figure()
   ```

5. data augmentation

   데이터에 랜덤한 변환을 적용하여 샘플을 늘리는 기법.

   ```python
   datagen = ImageDataGenerator(rotation_range=20,			# 회전
                                width_shift_range=0.1,		# 평행 이동
                                height_shift_range=0.1,	
                                shear_range=0.1,			# shearing transformation
                                zoom_range=0.1,			# 확대
                                horizontal_flip=True,		# 수평 전환
                                fill_mode='nearest')		# 회전이나 가로/세로 이동으로 인해
                                                            # 새롭게 생성해야할 픽셀 채울 전략
                                                            # nearest, constant, reflect, wrap
   ```

6. regularization

   Dropout을 Dense 층 앞에 추가하면 더 성능이 좋아질 가능성이 있음. Overfitting 방지.

#### 5-3. 사전 훈련된 ConvNet 사용

pretrained network를 사용하는 두 가지 방법. <b>특성 추출(feature extraction), 미세 조정(fine tuneing)</b>

1. 특성 추출

   사전 학습된 네트워크의 표현을 사용해서 새로운 샘플에서 특성을 뽑아내는 것. 이런 특성을 사용해서 새로운 분류기를 처음부터 훈련한다. 즉, 사전 학습된 네트워크의 Conv 층만 재사용한다.

   특정 Conv 층에서 추출한 표현의 일반성과 재사용성 수준은 층의 깊이에 따라 다르다. 모델의 하위 층은 (에지, 색, 질감 등) 지역적이고 매우 일반적인 특성 맵을 추출하고, 상위 층은 ('강아지 눈'이나 '고양이 귀' 등) 추상적인 개념을 추출한다. 따라서 새로운 데이터셋이 원본 모델이 훈련한 데이터셋과 많이 다르다면 모델의 하위 층 몇개만 특성 추출에 재사용하는 것이 좋다.

   keras.applications 모듈에서 임포트 할 수 있는 이미지 분류 모델

   - Xception		: "Xception: Deep Learning with Depthwise Separable Convolutions," arXiv (2016)
   - Inception V3  : "Rethinking the Inception Architecture for Computer Vision," arXiv (2015)
   - ResNet50      : "Deep Residual Learning for Image Recognition," arXiv (2015)
   - VGG16          : "Very Deep Convolutional Network for Large-Scale Image Recognition," arXiv (2014)
   - MobileNet      : "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications," arXiv (2017)
   - DenseNet      : "Densely Connected Convolutional Networks," arXiv (2016)
   - NASNet         : "Learning Transferable Architectures for Scalable Image Recognition," arXiv (2017)

   ```python
   from keras.applications import VGG16
   
   conv_base = VGG16(weights='imagenet',
                     include_top=False,
                     input_shape=(150, 150, 3))
   ```

   위의 코드에서 weights는 모델을 초기화할 가중치 체크포인트(checkpoint)를 지정. include_top은 네트워크의 최상위 완전 연결 분류기 포함 여부를 결정.

   사전 학습 모델을 이용해서 새로운 모델을 학습시키는 방법은 두 가지가 있다.

   1. 비용이 적은 방법 : 기존 모델에 새로운 데이터셋을 입력으로 실행해서 나온 출력은 NumPy 배열로 저장. 그 다음 독립된 분류기에 입력으로 넣어 학습. 이 방법은 전체 과정에서 가장 비용이 많이 드는 Conv 연산이 모든 입력 이미지에 대해 한 번만 실행하면 되므로 빠르고 비용이 적다. 하지만 data augmentation을 사용할 수 없다.

   2. end-to-end 방법 : 준비한 모델위에 새로운 분류기를 쌓아서 학습. 비용이 많이 든다.

      ```python
      from keras import models
      from keras import layers
      
      model = models.Sequential()
      mode.add(conv_base)
      model.add(layers.Flatten())
      model.add(layers.Dense(256, activation='relu'))
      model.add(layers.Dense(1, activation='sigmoid'))
      ```

      이 방법을 사용할 때 가장 중요한 것은 모델을 컴파일하고 훈련하기 전에 Conv_base를 동결하는 것이다. 층을 동결한다는 것은 훈련하는 동안 가중치가 업데이트되지 않도록 막는다는 뜻이며, 사전에 학습된 표현이 수정되지 않도록 하는 방법이다.

      keras에서는 trainable 속성을 False로 설정하면 된다.

      ```python
      conv_base.trainable = False
      ```

2. 미세 조정 (fine tuning)

   특성 추출에 사용했던 동결 모델의 상위 층 몇개를 동결에서 해제하고 모델에 새로 추가한 층과 함께 훈련하는 것.
   주어진 문제에 조금 더 밀접하게 재사용 모델의 표현을 일부 조정하기 때문에 미세 조정이라고 함.

   이 때 분류기를 미리 훈련하지 않으면 너무 큰 오차 신호가 전파되어 사전에 학습한 표현들이 망가질 수 있다. 그래서 학습 방법은 다음과 같다.

   - 사전에 훈련된 기반 네트워크 위에 새로운 네트워크를 추가
   - 기반 네트워크를 동결
   - 새로 추가한 네트워크 훈련
   - 기반 네트워크에서 일부 층의 동결을 해제
   - 동결을 해제한 층과 새로 추가한 층을 함께 훈련

   ```python
   conv_base.summary() # summary 메소드를 통해 기본 모델의 구성 layer을 확인할 수 있음.
   
   conv_base.trainable = True
   
   set_trainable = False
   for layer in conv_base.layers:
       if layer.name == 'block5_conv1':
           set_trainable = True
           
       if set_trainable:
           layer.trainable = True
       else:
           layer.trainable = False
   ```

> <b>Q. 손실이 감소되지 않았는데 어떻게 정확도가 안정되거나 향상될 수 있을까?</b>
>
> 손실은 평균이다. 하지만 정확도에 영향을 미치는 것은 손실 값의 분포이지 평균이 아니다. 정확도는 모델이 예측한 클래스 확률이 어떤 임계 값을 넘었는지에 대한 결과이다. 따라서 모델이 더 향상되더라도 평균 손실에 반영되지 않을 수 있다.

#### 5-4. ConvNet 학습 시각화

- ConvNet 중간층의 출력
- ConvNet 필터를 시각화
- 클래스 활성화에 대한 히트맵을 이미지에 시각화

관련 정리한 내용은 <a href="../../study/_posts/2019-09-20-MACHINE_LEARNING-ConvNet-학습-시각화.md">MACHINE_LEARNING - ConvNet 학습 시각화</a>에서 참고.

### &#128218; 6장. 텍스트와 시퀀스를 위한 딥러닝

#### 6-1. 텍스트 데이터 다루기

텍스트는 가장 흔한 시퀀스 형태의 데이터. 시퀀스 데이터를 처리하는 기본적인 딥러닝 모델은 <b>순환 신경망(recurrent neural network)과 1D 컨브넷(1D convnet)</b>.

모델을 학습시킬 때 텍스트 원본을 입력으로 사용하지 못함. 딥러닝 모델은 수치형 텐서만 다룰 수 있기 때문. 텍스트를 수치형 텐서로 변환하는 과정을 <b>텍스트 벡터화(vectorizing)</b>라고 함. 다양한 방법이 존재.

- 텍스트를 단어로 나누고 각 단어를 하나의 벡터로 변환
- 텍스트를 문자로 나누고 각 문자를 하나의 벡터로 변환
- 텍스트에서 단어나 문자의 n-그램(n-gram)을 추출하여 각 n-그램을 하나의 벡터로 변환. n-그램은 연속된 단어나 문자의 그룹으로 텍스트에서 단어나 문자를 하나씩 이동하면서 추출.

텍스트를 나누는 단위(단어, 문자, n-그램)을 <b>토큰(token)</b>이라고 함. 토큰으로 나누는 작업은 토큰화(tokenization)
토큰과 벡터를 연결하는 방법은 여러가지가 있는데, 주요 두 가지 방법은 <b>원-핫 인코딩(one-hot encoding)과 토큰 임베딩(token embedding) 혹은 단어 임베딩(word embedding)</b>이다.

> <b>n-그램과 BoW</b>
>
> 단어 n-그램은 문장에서 추출한 N개(또는 그 이하)의 연속된 단어 그룹이다. 단어 대신 문자에도 적용 가능.
> 만약 "The cat sat on the mat."이란 문장이 있을 때 2-그램의 집합으로 분해한다면 다음과 같다.
>
> <p align="center">{"The", "The cat", "cat", "cat sat", "sat", "sat on", "on", "on the", "the", "the mat", "mat"}</p>
>또 다음 3-그램의 집합으로 분해한다면 다음과 같다.
> 
><p align="center">{"The", "The cat", "cat", "cat sat", "The cat sat", "sat", "sat on", "on", "cat sat on", "on the", "the", "sat on the", "the mat", "mat", "on the mat"}</p>
> 이런 집합을 각각 bag of 2-gram, bag of 3-gram이라고 한다. 이런 종류의 토큰화 방법을 BoW(Bag-of-Words).
>
> BoW는 순서가 없는 토큰화 방법이기 때문에 얕은 학습 방법의 언어 처리 모델에 사용되는 경향이 있음. 일종의 특성 공학.

##### 단어 임베딩 (word embedding)

단어 임베딩은 밀집 단어 벡터를 사용하는 것. 원-핫 인코딩으로 만든 벡터는 sparse하고 고차원이지만, 단어 임베딩은 저차원의 실수형 밀집 벡터이다. 원-핫 인코딩으로 얻은 단어 벡터와 달리 단어 임베딩은 데이터로부터 학습된다. 보통 256차원, 512차원 또는 큰 어휘 사전을 다룰 때는 1,024차원이 단어 임베딩을 사용한다.

만드는 방법은 두 가지이다.

- 관심 대상인 문제와 함께 단어 임베딩을 학습. 랜덤한 단어 벡터로 시작해서 가중치 학습.
- 다른 머신러닝에서 미리 계산된 단어 임베딩을 로드. pretrained word embedding

단어 벡터 간에 추상적인 관계를 얻으려면 단어 사이에 있는 의미 관계를 반영해야 한다. 단어 임베딩은 언어를 기하학적 공간에 매핑하는 것이다. 예를 들어 잘 구축된 임베딩 곤간에서는 동의어가 비슷한 단어 벡터로 임베딩 된다. 또한 벡터로 표현할 수 있기 때문에, 어떤 특성을 빼거나 더해서 다른 벡터로 연결할 수도 있다.

```python
from keras.layers import Embedding

# Embedding 층은 가능한 토큰의 개수와 임베딩차원 이렇게 2개의 매개변수를 받는다.
# (samples, sequence_length, embedding_dimensionality)인 3D 실수형 텐서 반환.
embedding_layer = Embedding(1000, 64)
```

단어 임베딩을 구하는 여러가지 기법

- <a href="https://code.google.com/archive/p/word2vec">Word2vec</a>

- <a href="https://nlp.stanford.edu/projects/glove">GloVe(Global Vectors for Word Representation)</a> : 단어의 동시 출현(co-occurrence) 통계를 기록한 행렬을 분해하는 기법 사용.

  > <b>co-occurrence</b>
  >
  > 언어학에서 형태소나 음소가 올바른 문법의 문장 안에 동시에 나타나는 것.

##### 데이터 토큰화

```python
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import numpy as np

maxlen = 100
training_samples = 200
validation_samples = 10000
max_words = 10000 # 빈도 높은 1만개 단어만 사용

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

word_index = tokenizer.word_index
data = pad_sequences(sequences, maxlen=maxlen)
```

#### 6-2. 순환 신경망 이해하기

글을 이용해 문제를 풀기 위해서는 입력이 개별적으로 처리되지 않고 연속성을 유지할 수 있는, 즉 상태가 유지되는 네트워크가 필요하다. <b>순환 신경망(Recurrent Neural Network, RNN)</b>이 이러한 문제를 해결하는데 사용되는 가장 기본적인 모델이다. 이 모델은 시퀀스의 원소를 순회하면서 지금까지 처리한 정보를 상태(state)에 저장한다. 다시 말해 한 개의 데이터 포인트가 한 번에 처리되지 않고, 시퀀스의 원소를 차례대로 돌면서 처리한다.

![RNNflow](https://user-images.githubusercontent.com/27988544/65310453-4e4ba300-dbc9-11e9-92e4-00af54e04d85.jpg)

<p align="center">RNN model</p>
- RNN은 (timesteps, input_features)인 2D 텐서로 인코딩된 벡터의 시퀀스를 입력으로 사용.
- 시퀀스는 타임스텝을 따라서 반복.
- 각 타임스텝 t에서 현재 상태와 (input_features,) 크기의 입력을 연결하여 출력을 계산.
- 그리고 이 출력을 다음 스텝의 상태로 설정.
- 첫 번째 타임스텝에서는 이전 출력이 정의되지 않으므로 초기 상태(initial state)인 0 벡터로 초기화.

```python
##########
# numpy로 RNN 구현
import numpy as np

timesteps = 100
input_features = 32
output_features = 64

# 입력 데이터
inputs = np.random.random((timesteps, input_features))

# 초기 상태: 모두 0인 벡터
state_t = np.zeros((output_features, ))

# 가중치
W = np.random.random((output_features, input_features))
U = np.random.random((output_features, output_features))
b = np.random.random((output_faetures, ))

successive_outputs = []
for input_t in inputs:
    # 입력과 현재 상태(이전 출력)를 연결해서 현재 출력을 얻는다.
    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)
    successive_outputs.append(output_t)
    state_t = output_t

# 최종 출력 : (timesteps, output_features)
final_output_sequence = np.stack(successive_outputs, axis=0)
```

출력 텐서은 각 타임스텝 0에서 t까지 전체 과거에 대한 정보를 담고 있다. 우리는 전체 시퀀스에 대한 정보가 필요하기 때문에 마지막 출력만 사용한다.

##### 케라스의 순환 층

위에서 넘파이로 구현한 내용이 케라스의 SimpleRNN 층에 해당한다.

```python
from keras.layers import SimpleRNN
```

SimpleRNN이 한 가지 다른 점은 넘파이 예제처럼 하나의 시퀀스가 아니라 다른 케라스 층과 마찬가지로 시퀀스 배치를 처리한다는 것. 즉, (batch_size, timesteps, input_features) 크기의 입력을 받는다.

케라스에 있는 모든 순환 층과 마찬가지로 SimpleRNN은 두 가지 모드로 실행할 수 있다. 각 타임스텝의 출력을 모은 전체 시퀀스를 반환하거나 (출력 크기가 (batch_size, timesteps, output_features)인 3D 텐서), 입력 시퀀스에 대한 마지막 출력만 반환할 수 있다. (출력 크기가 (batch_size, output_features)인 2D 텐서) 이 모드는 객체를 생성할 때 return_sequences 매개변수로 선택할 수 있다.

```python
model.add(Embedding(10000, 32))
model.add(SimpleRNN(32, return_sequences=True)) # 마지막 타임스텝의 출력만 반환
model.add(SimpleRNN(32)) # 전체 타임스텝의 출력 반환
```

> 층의 이름은 클래스 이름에서 단어 사이에 언더바를 추가하고 소문자로 바꾸어 사용한다. 이름 뒤에 붙는 숫자는 클래스별로 1씩 증가된다. 별도의 이름을 주려면 매개변수로 name을 설정한다.

네트워크의 표현력을 증가시키기 위해 여러 개의 순환 층을 쌓는 것이 유용할 때가 있다. 이런 설정에서는 중간층들이 <b>전체 출력 시퀀스를 반환</b>하도록 설정해야 한다.

##### LSTM과 GRU 층 이해하기

케라스에는 LSTM과 GRU라는 다른 순환 layer도 존재. RNN은 이론적으로는 이전의 모든 타임스텝에 대한 정보를 유지해야하나 실제로는 학습할 수 없다는 문제가 있다. <b>기울기 소실 문제(vanishing gradient problem)</b> 때문인데, 이 문제를 해결하기 위해 고안도니 것이 LSTM과 GRU이다.

LSTM(Long Short-Term Memory)는 정보를 여러 타임스텝에 걸쳐 나르는 방법이 추가되었다.
![LSTMflow](https://user-images.githubusercontent.com/27988544/65310734-ee093100-dbc9-11e9-9e6d-590767ccd741.jpg)

위의 LSTM 구조를 sudo code로 표현하면 다음과 같다.

```python
output_t = activation(c_t) * activation(dot(input_t, Wo) + dot(state_t, Uo) + bo)

i_t = activation(dot(state_t, Ui) + dot(input_t, Wi) + bi)
f_t = activation(dot(state_t, Uf) + dot(input_t, Wf) + bf)
k_t = activation(dot(state_t, Uk) + dot(input_t, Wk) + bk)

c_{t+1} = i_t * k_t + c_t * f_t
```

먼저 c_t와 f_t의 곱셈은 이동을 위한 데이터 흐름에서 관련이 적은 정보를 의도적으로 삭제한다고 볼 수 있다.
한편 i_t와 k_t는 현재에 대한 정보를 제공하고, 이동 트랙을 새로운 정보로 업데이트한다고 볼 수 있다.

이런 해석 때문에 f_t의 계산식을 삭제 게이트(forget gate), i_t의 계산식을 입력 게이트(input gate)라고 부른다. 하지만 결국 이런 해석은 큰 의미가 없다. 이 연산들이 실제로 하는 일은 연산에 관련된 가중치 행렬에 따라 결정되기 때문이다.

요약하면 <b>LSTM 셀의 구체적인 구조에 대해 이해할 필요가 없다. 과거 정보를 나중에 다시 주입해서 gradient vanishing 문제를 해결한다는 점이 중요하다.</b>

```python
from keras.layrers import LSTM

model = Sequential()
model.add(Embedding(max_features, 32))
model.add(LSTM(32))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['acc'])
history = model.fit(input_train, y_train,
                    epochs=10,
                    batch_size=128,
                    validation_split=0.2)
```

#### 6-3. 순환 신경망의 고급 사용법

1. 순환 드롭아웃 (recurrent dropout)

   ```python
   
   ```

2. 스태킹 순환 층 (stacking recurrent layer)

   네트워크의 표현 능력을 증가시킨다. 대신 비용이 증가함.

3. 양뱡향 순환 층(bidirectional recurrent layer)

   같은 정보를 다른 방향으로 주입해서 정확도를 높이고 기억을 좀 더 오래 유지.